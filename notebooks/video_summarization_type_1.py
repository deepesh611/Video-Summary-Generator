# -*- coding: utf-8 -*-
"""Video Summarization Type-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_jzCrONdlxyJZvT6kax4njHAo9g0mAwm
"""

import torch
print(torch.cuda.is_available())

# !pip install opencv-python torch torchvision transformers

import os
import cv2
import glob
import json
import torch
import requests
import torchvision
import numpy as np
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms

from PIL import Image
from google.colab import files
from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline


# Frame Extraction
video_path = r"/content/drive/MyDrive/Video-to-Text Summarization/Set-2/VID-20251019-WA0001.mp4"
output_dir = r"/content/drive/MyDrive/Video-to-Text Summarization/frames"

os.makedirs(output_dir, exist_ok=True)

vidcap = cv2.VideoCapture(video_path)
success, image = vidcap.read()
count = 0
frame_skip = 30

while success:
    if count % frame_skip == 0:
        cv2.imwrite(os.path.join(output_dir, f"frame_{count}.jpg"), image)
    success, image = vidcap.read()
    count += 1

print(f"Extracted {len(os.listdir(output_dir))} frames to '{output_dir}'")


# Load pretrained ResNet (remove last fully-connected layer)
resnet = models.resnet50(weights=True)  # You can use resnet18 for faster/slimmer model
resnet.eval()  # Set to evaluation mode
feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove final classification layer

# Image preprocessing pipeline for ResNet
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(  # Standard ImageNet normalization
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225])
])

frame_files = sorted(glob.glob("/content/drive/MyDrive/Video-to-Text Summarization/frames/frame_*.jpg"))
features = []

for path in frame_files:
    image = Image.open(path).convert("RGB")
    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension

    with torch.no_grad():
        output = feature_extractor(input_tensor)  # Shape: [1, 2048, 1, 1]
        vector = output.squeeze().cpu().numpy()   # Shape: [2048]

    features.append(vector)


# Frame Importance Scoring
features = np.array(features)
print(f"Extracted feature vectors for {len(features)} frames, vector shape: {features[0].shape}")

class FrameImportanceLSTM(nn.Module):
    def __init__(self, input_size=2048, hidden_size=256, num_layers=2):
        super(FrameImportanceLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size*2, 1)  # Output: score for each frame

    def forward(self, x):
        h, _ = self.lstm(x)
        scores = torch.sigmoid(self.fc(h)).squeeze(-1)
        return scores

# Instantiate the model
model = FrameImportanceLSTM()
model.eval()

features_tensor = torch.tensor(features).unsqueeze(0).float()
with torch.no_grad():
    importance_scores = model(features_tensor).squeeze().cpu().numpy()  # [num_frames]

# Select key frames above a threshold
import numpy as np
threshold = 0.5
key_indices = np.where(importance_scores > threshold)[0]
key_frames = [frame_files[i] for i in key_indices]
print("Selected key frames for summary:", key_frames)


# Frame Captioning
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

def caption_frame(image_path, processor, model):
    raw_image = Image.open(image_path).convert('RGB')
    inputs = processor(raw_image, return_tensors="pt")
    with torch.no_grad():
        out = model.generate(**inputs)
        caption = processor.decode(out[0], skip_special_tokens=True)
    return caption



# Generate and print summaries
video_summary = []
for idx, frame_path in enumerate(key_frames):
    caption = caption_frame(frame_path, processor, caption_model)
    video_summary.append(f"Scene {idx+1}: {caption}")
    print(f"Scene {idx+1}: {caption}")

with open("/content/drive/MyDrive/Video-to-Text Summarization/video_summary.txt", "w") as f:
    for line in video_summary:
        f.write(line + "\n")

summarizer = pipeline("summarization", model="tngtech/deepseek-r1t2-chimera:free")

f = open("/content/drive/MyDrive/Video-to-Text Summarization/video_summary.txt", "r")
summary_raw = f.read()

api_url = os.getenv("API_URL")
api_key = os.getenv("API_KEY")

response = requests.post(
    api_url,
    headers = {
        "Authorization":f"Bearer {api_key}",
    },
    data = json.dumps({
        "model":"tngtech/deepseek-r1t2-chimera:free",
        "messages":[{
            "role":"user",
            "content": f'''
            Rewrite and summarize this scientific video breakdown for a general audience. Only give the summary, no other intro or outro.:

            {summary_raw}''',
      }]
    })
)

print(response.json())

print("AI summary:\n", response.json()['choices'][0]['message']['content'])